{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compolsory Assignment 4: Variational Autoencoders\n",
    "Please fill out the the group name, number, members and optionally the name below.\n",
    "\n",
    "**Group number**: \\\n",
    "**Group member 1**: \\\n",
    "**Group member 2**: \\\n",
    "**Group name (optional)**:\n",
    "\n",
    "# Assignment Submission\n",
    "To complete this assignment answer the relevant questions in this notebook and write the code required to implement the relevant models. It concist of **<u>two tasks</u>**. At the end of each task, there are discussion questions that _must_ be answered. The assignemnt is submitted by handing in this notebook as an .ipynb file and as a .pdf file. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will explore autoencoder networks using the **Sign Language MNIST** dataset from Kaggle (as same as CA1). It contains 28Ã—28 grayscale images of hand signs representing letters Aâ€“Y (excluding J and Z), for 24 classes in total. The data is provided as CSV files with a label column and 784 pixel columns. The standard split includes 27,455 training images and 7,172 test images\n",
    "\n",
    "Autoencoder is an introduction to encoder-decoder architecture. Here we will use an encoder to transform the data into an latent representation, and a decoder to project it back into the visual space.\n",
    "Two main components: an **encoder** and a **decoder**.\n",
    " - The encoder takes in the input data and maps it to a lower-dimensional latent representation. It typically consists of multiple layers that gradually reduce the dimensionality of the input data, capturing its essential features. The output of the encoder is a compressed representation of the input, often referred to as a code or latent vector.\n",
    " - The decoder, on the other hand, takes the code from the encoder and reconstructs the original data from it. It mirrors the architecture of the encoder by gradually expanding the code back to the original dimensionality. The output of the decoder is a reconstruction of the input data, which ideally should closely resemble the original input."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div>\n",
    "<img src=\"https://tikz.net/janosh/autoencoder.png\" width=\"300\"/>\n",
    "<div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Task 1**\n",
    "- Load the SignMNIST dataset.\n",
    "\t- > Use the provided CSV files sign_mnist_train.csv and sign_mnist_test.csv.\n",
    "\t- Normalize the images to have pixel values between 0 and 1.\n",
    "- Build an autoencoder model.\n",
    "\t- The encoder should compress the images into a lower-dimensional latent space.\n",
    "\t- The decoder should reconstruct the images from the latent space.\n",
    "\t- The intermediat layers can either be linear or convolutional.\n",
    "- Train the autoencoder.\n",
    "\t- Use `binary-cross-entropy` loss function for _reconstruction_ (i.e. target value = input value)\n",
    "\t- You can choose optimizer freely (Adam is recommended).\n",
    "- Visualize the reconstructed images.\n",
    "\t- _Compare_ original and reconstructed images.\n",
    "- Visualize the latent space\n",
    "\t- Use PCA to visualize the 2d-latent space.\n",
    "\t- Use the digit as labels\n",
    "- Answer the discussion questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Load and preprocess the Sign MNIST dataset\n",
    "# Your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Build the autoencoder model\n",
    "# Your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Compile and train the autoencoder\n",
    "# Your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Visualize the reconstructed images\n",
    "# Your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Visualize the PCA-projection of the latent space. Try to inlcude the labels in the plot for a better understanding of \n",
    "# the distribution of the latent space.\n",
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Discussion**\n",
    "1. Why do we normalize the pixel values of the images?\n",
    "2. How does the size of the latent space affect the reconstructed images?\n",
    "3. Can autoencoders be used for noise reduction? How?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discuss here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "### **Task 2**\n",
    "\n",
    "In contrast to simple autoencoders, which aim to learn a deterministic mapping from input data to a compressed latent space and back, **Variational Autoencoders (VAEs)** introduce a probabilistic approach to the latent space. While simple autoencoders can reconstruct data effectively, they are not inherently capable of generating new, realistic data samples. VAEs, on the other hand, are designed for _both reconstruction and generation_ of novel data samples, making them highly useful for generative modeling tasks.\n",
    "\n",
    "**Why VAEs?** A simple autoencoder maps input data to a fixed point in the latent space, meaning that each input has a single corresponding latent vector. While this approach works well for compression and reconstruction, it lacks the ability to generalize and generate new data points because the latent space is not continuous or well-structured.\n",
    "\n",
    "In contrast, a Variational Autoencoder treats the latent space probabilistically. Instead of mapping each input to a single point, it maps inputs to a distribution (typically a Gaussian distribution) in the latent space. This probabilistic treatment allows for the generation of new samples by sampling from the learned distribution, thus enabling the model to create *new*, unseen data points that resemble the training data.\n",
    "\n",
    "**How VAEs Work?** In a VAE, the _encoder_ maps the input data to <u>two</u> vectors: a _mean vector_ and a _variance (or log variance) vector_ (in contrast to one as in a traditional AE). These vectors representing a Gaussian distribution in the latent space. Instead of encoding to a fixed point, the model <u>samples</u> a latent vector from this distribution, using these mean and variance parameters. This sampling introduces variability, allowing the VAE to explore different parts of the latent space and generate diverse outputs.\n",
    "\n",
    "The _decoder_ then reconstructs the data by decoding from the sampled latent vector, which can either come from the encoded input or be randomly generated. This process makes VAEs ideal for generating novel data samples in applications like image generation.\n",
    "\n",
    "**The loss function?** The loss function plays a crucial role in training the model to both reconstruct input data and ensure that the latent space is properly structured. The VAE loss function consists of two main components:\n",
    "# 1. Reconstruction Loss\n",
    "\n",
    "Measures how well the VAE is able to reconstruct the input data from the latent space.  \n",
    "Typically, this is computed using **binary cross-entropy** (for normalized image data) or **mean squared error** (for continuous data).  \n",
    "For images, binary cross-entropy is often used.\n",
    "\n",
    "$$\n",
    "\\mathcal{L}_{\\text{reconstruction}}(x, \\hat{x}) = \n",
    "- \\sum_{i=1}^{N} \\left[ x_i \\log(\\hat{x}_i) + (1 - x_i)\\log(1 - \\hat{x}_i) \\right]\n",
    "$$\n",
    "\n",
    "Where:  \n",
    "- $x_i$ is the original data point (e.g., pixel value)  \n",
    "- $\\hat{x}_i$ is the reconstructed data point  \n",
    "- $N$ is the number of data points or pixels  \n",
    "\n",
    "---\n",
    "\n",
    "# 2. KL Divergence Loss\n",
    "\n",
    "Ensures that the learned latent distribution $q(z|x)$, parameterized by $\\mu$ and $\\sigma^2$,  \n",
    "is close to a standard normal distribution $p(z) = \\mathcal{N}(0, I)$.\n",
    "\n",
    "$$\n",
    "\\mathcal{L}_{\\text{KL}}(q(z|x) \\parallel p(z)) = \n",
    "\\frac{1}{2} \\sum_{j=1}^{d} \n",
    "\\left( 1 + \\log(\\sigma_j^2) - \\mu_j^2 - \\sigma_j^2 \\right)\n",
    "$$\n",
    "\n",
    "Where:  \n",
    "- $\\mu_j$ is the mean of the latent variable distribution  \n",
    "- $\\sigma_j^2$ is the variance of the latent variable distribution  \n",
    "- $d$ is the dimensionality of the latent space  \n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ§® Total VAE Loss\n",
    "\n",
    "The total loss function for a VAE is the sum of the reconstruction loss and the KL divergence loss:\n",
    "\n",
    "$$\n",
    "\\mathcal{L}_{\\text{VAE}} = \n",
    "\\mathcal{L}_{\\text{reconstruction}} + \\mathcal{L}_{\\text{KL}}\n",
    "$$\n",
    "\n",
    "\n",
    "**The Role of KL Divergence** A crucial aspect of training VAEs is ensuring that the latent space is well-structured and that the learned distributions align with a _prior distribution_ (typically a standard normal distribution). This is where the _Kullback-Leibler (KL) divergence loss_ comes in. KL divergence measures how different the learned distribution is from the desired prior distribution. By minimizing KL divergence during training, the VAE ensures that the latent space follows a continuous, smooth distribution, making it easier to generate realistic new samples by sampling from this space.\n",
    "\n",
    "In summary, VAEs combine the strengths of autoencoders for reconstruction with a probabilistic latent space, enabling the generation of new data samples and ensuring smooth interpolation within the latent space through the use of KL divergence. This combination makes VAEs a powerful generative model in unsupervised learning tasks.\n",
    "\n",
    "<div>\n",
    "<img src=\"https://tikz.net/janosh/vae.png\" width=\"350\"/>\n",
    "<div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the following code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpd\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtf\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mkeras\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m layers, Model, backend \u001b[38;5;28;01mas\u001b[39;00m K\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmatplotlib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpyplot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mplt\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/repositories/github.com/ioeftedal/dat300/.venv/lib/python3.12/site-packages/tensorflow/__init__.py:438\u001b[39m\n\u001b[32m    436\u001b[39m _plugin_dir = _os.path.join(_s, \u001b[33m\"\u001b[39m\u001b[33mtensorflow-plugins\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    437\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m _os.path.exists(_plugin_dir):\n\u001b[32m--> \u001b[39m\u001b[32m438\u001b[39m   \u001b[43m_ll\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload_library\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_plugin_dir\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    439\u001b[39m   \u001b[38;5;66;03m# Load Pluggable Device Library\u001b[39;00m\n\u001b[32m    440\u001b[39m   _ll.load_pluggable_device_library(_plugin_dir)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/repositories/github.com/ioeftedal/dat300/.venv/lib/python3.12/site-packages/tensorflow/python/framework/load_library.py:151\u001b[39m, in \u001b[36mload_library\u001b[39m\u001b[34m(library_location)\u001b[39m\n\u001b[32m    148\u001b[39m     kernel_libraries = [library_location]\n\u001b[32m    150\u001b[39m   \u001b[38;5;28;01mfor\u001b[39;00m lib \u001b[38;5;129;01min\u001b[39;00m kernel_libraries:\n\u001b[32m--> \u001b[39m\u001b[32m151\u001b[39m     \u001b[43mpy_tf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mTF_LoadLibrary\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlib\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    153\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    154\u001b[39m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\n\u001b[32m    155\u001b[39m       errno.ENOENT,\n\u001b[32m    156\u001b[39m       \u001b[33m'\u001b[39m\u001b[33mThe file or folder to load kernel libraries from does not exist.\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m    157\u001b[39m       library_location)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/repositories/github.com/ioeftedal/dat300/.venv/lib/python3.12/site-packages/tensorflow/python/framework/errors_impl.py:313\u001b[39m, in \u001b[36mNotFoundError.__init__\u001b[39m\u001b[34m(self, node_def, op, message, *args)\u001b[39m\n\u001b[32m    303\u001b[39m \u001b[38;5;129m@tf_export\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33merrors.NotFoundError\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    304\u001b[39m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mNotFoundError\u001b[39;00m(OpError):\n\u001b[32m    305\u001b[39m \u001b[38;5;250m  \u001b[39m\u001b[33;03m\"\"\"Raised when a requested entity (e.g., a file or directory) was not found.\u001b[39;00m\n\u001b[32m    306\u001b[39m \n\u001b[32m    307\u001b[39m \u001b[33;03m  For example, running the\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    310\u001b[39m \u001b[33;03m  does not exist.\u001b[39;00m\n\u001b[32m    311\u001b[39m \u001b[33;03m  \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m313\u001b[39m   \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, node_def, op, message, *args):\n\u001b[32m    314\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Creates a `NotFoundError`.\"\"\"\u001b[39;00m\n\u001b[32m    315\u001b[39m     \u001b[38;5;28msuper\u001b[39m(NotFoundError, \u001b[38;5;28mself\u001b[39m).\u001b[34m__init__\u001b[39m(node_def, op, message, NOT_FOUND, *args)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: ",
      "Normalization failed: type=NotFoundError args=(None, None, \"dlopen(/Users/ivareftedal/repositories/github.com/ioeftedal/dat300/.venv/lib/python3.12/site-packages/tensorflow-plugins/libmetal_plugin.dylib, 0x0006): Library not loaded: @rpath/_pywrap_tensorflow_internal.so\\n  Referenced from: <8B62586B-B082-3113-93AB-FD766A9960AE> /Users/ivareftedal/repositories/github.com/ioeftedal/dat300/.venv/lib/python3.12/site-packages/tensorflow-plugins/libmetal_plugin.dylib\\n  Reason: tried: '/Users/ivareftedal/repositories/github.com/ioeftedal/dat300/.venv/lib/python3.12/site-packages/tensorflow-plugins/../_solib_darwin_arm64/_U@local_Uconfig_Utf_S_S_C_Upywrap_Utensorflow_Uinternal___Uexternal_Slocal_Uconfig_Utf/_pywrap_tensorflow_internal.so' (no such file), '/Users/ivareftedal/repositories/github.com/ioeftedal/dat300/.venv/lib/python3.12/site-packages/tensorflow-plugins/../_solib_darwin_arm64/_U@local_Uconfig_Utf_S_S_C_Upywrap_Utensorflow_Uinternal___Uexternal_Slocal_Uconfig_Utf/_pywrap_tensorflow_internal.so' (no such file)\", {})"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, Model, backend as K\n",
    "import matplotlib.pyplot as plt\n",
    "import keras\n",
    "from keras import ops, layers\n",
    "from tensorflow.keras.layers import Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the MNIST dataset.\n",
    "train_df = pd.read_csv(\"sign_mnist_train.csv\")\n",
    "test_df  = pd.read_csv(\"sign_mnist_test.csv\")\n",
    "\n",
    "x_train = train_df.drop(columns=['label']).to_numpy().astype('float32') / 255.0\n",
    "x_test  = test_df.drop(columns=['label']).to_numpy().astype('float32') / 255.0\n",
    "\n",
    "x_train = np.reshape(x_train, (len(x_train), 28, 28, 1))\n",
    "x_test  = np.reshape(x_test,  (len(x_test), 28, 28, 1))\n",
    "\n",
    "y_train = train_df['label'].to_numpy()\n",
    "y_test  = test_df['label'].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_dim = 2  # Dimensionality of the latent space (small for simplicity)\n",
    "input_shape = (28, 28, 1)  # Input image shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We define the encoder in a similar way as the autoencoder\n",
    "input_img = layers.Input(shape=input_shape)\n",
    "x = layers.Conv2D(32, 3, activation='relu', padding='same')(input_img)\n",
    "x = layers.MaxPooling2D(2, padding='same')(x)\n",
    "x = layers.Conv2D(64, 3, activation='relu', padding='same')(x)\n",
    "x = layers.MaxPooling2D(2, padding='same')(x)\n",
    "x = layers.Flatten()(x)\n",
    "x = layers.Dense(128, activation='relu')(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# However, instead of a single output layer to the latent space, we have two output layers (latent space parameters: mean (mu) and log variance (log_var))\n",
    "z_mean = layers.Dense(latent_dim)(x)\n",
    "z_log_var = layers.Dense(latent_dim)(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We define a sampling function to sample from the latent space, which introduces stochasticity in the model and helps in training (as it still alows gradients to flow backwards)\n",
    "# Sampling function\n",
    "def sampling(args):\n",
    "    z_mean, z_log_var = args\n",
    "    epsilon = K.random_normal(shape=K.shape(z_mean))\n",
    "    return z_mean + K.exp(0.5 * z_log_var) * epsilon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# There are multiple ways to sample from the latent space, here I introduce a Lambda layer, which allows to use arbitrary functions as layers in the model\n",
    "# Note the output shape of the Lambda layer; we are returning the (sampled) latent space, and the learned parameters (mean and log variance)- these are used in the loss function\n",
    "z = layers.Lambda(sampling, output_shape=(latent_dim,))([z_mean, z_log_var])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The decoder is defined in the same way as the autoencoder\n",
    "decoder_input = layers.Input(shape=(latent_dim,))\n",
    "d = layers.Dense(7 * 7 * 64, activation='relu')(decoder_input)\n",
    "d = layers.Reshape((7, 7, 64))(d)\n",
    "d = layers.Conv2DTranspose(64, 3, activation='relu', strides=2, padding='same')(d)\n",
    "d = layers.Conv2DTranspose(32, 3, activation='relu', strides=2, padding='same')(d)\n",
    "decoded_img = layers.Conv2DTranspose(1, 3, activation='sigmoid', padding='same')(d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a Variational Autoencoder (VAE), adjusting the weight of the KL divergence loss controls the balance between data reconstruction accuracy and latent space regularization. Increasing the weight encourages a more structured and continuous latent space but may reduce reconstruction quality, while decreasing it improves reconstruction but risks losing meaningful structure in the latent space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#KL divergence loss weight. This is a hyperparameter that can be tuned, run ALL cells below to see the effect of different values.\n",
    "# This is the value you should experiment with\n",
    "KL_weight = 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Having build our layers, we can now define the models\n",
    "encoder = Model(input_img, [z_mean, z_log_var, z], name='encoder')\n",
    "decoder = Model(decoder_input, decoded_img, name='decoder')\n",
    "\n",
    "# VAE model that connects the encoder and decoder\n",
    "vae_output = decoder(encoder(input_img)[2])\n",
    "vae = Model(input_img, vae_output, name='vae')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VAE Loss Layer\n",
    "# Variables: x (input), x_hat (reconstruction), mu (z_mean), logvar (z_log_var)\n",
    "\n",
    "class VAE_LossLayer(Layer):\n",
    "    def __init__(self, beta=1.0, rec_kind=\"bce\", **kwargs):\n",
    "        super(VAE_LossLayer, self).__init__(**kwargs)\n",
    "        self.beta = beta\n",
    "        self.rec_kind = rec_kind\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x, x_hat, mu, logvar = inputs\n",
    "\n",
    "        # Reconstruction loss \n",
    "        if self.rec_kind == \"bce\":\n",
    "            rec_loss = -tf.reduce_sum(\n",
    "                x * tf.math.log(tf.clip_by_value(x_hat, 1e-7, 1.0)) +\n",
    "                (1 - x) * tf.math.log(tf.clip_by_value(1 - x_hat, 1e-7, 1.0)),\n",
    "                axis=[1, 2, 3]\n",
    "            )\n",
    "        elif self.rec_kind == \"mse\":\n",
    "            rec_loss = tf.reduce_sum(tf.square(x - x_hat), axis=[1, 2, 3])\n",
    "        else:\n",
    "            raise ValueError(\"rec_kind must be 'bce' or 'mse'\")\n",
    "\n",
    "        # KL Divergence\n",
    "        kl_loss = -0.5 * tf.reduce_sum(1 + logvar - tf.square(mu) - tf.exp(logvar), axis=1)\n",
    "\n",
    "        # Total loss \n",
    "        total_loss = tf.reduce_mean(rec_loss + self.beta * kl_loss)\n",
    "\n",
    "        self.add_loss(total_loss)\n",
    "        return x_hat  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train the VAE \n",
    "\n",
    "lossed_output = VAE_LossLayer(beta=1.0, rec_kind=\"bce\")(\n",
    "    [input_img, vae_output, z_mean, z_log_var]\n",
    ")\n",
    "\n",
    "vae = keras.Model(inputs=input_img, outputs=lossed_output, name=\"vae\")\n",
    "\n",
    "vae.compile(optimizer=\"adam\")\n",
    "\n",
    "#Feel free to play around with the paramaters\n",
    "history = vae.fit(\n",
    "    x_train,\n",
    "    epochs=25,\n",
    "    batch_size=128,\n",
    "    validation_data=(x_test,),\n",
    "    verbose=2,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this visualisation code after training the VAE to answere the discussion questions.\n",
    "def plot_latent_space(decoder, n=15, figsize=15):\n",
    "    \"\"\"Plots a grid of digits decoded from the VAE's latent space.\"\"\"\n",
    "    scale = 2.0\n",
    "    figure = np.zeros((28 * n, 28 * n))\n",
    "    grid_x = np.linspace(-scale, scale, n)\n",
    "    grid_y = np.linspace(-scale, scale, n)\n",
    "\n",
    "    for i, yi in enumerate(grid_y):\n",
    "        for j, xi in enumerate(grid_x):\n",
    "            z_sample = np.array([[xi, yi]])\n",
    "            x_decoded = decoder.predict(z_sample)\n",
    "            digit = x_decoded[0].reshape(28, 28)\n",
    "            figure[i * 28: (i + 1) * 28, j * 28: (j + 1) * 28] = digit\n",
    "\n",
    "    plt.figure(figsize=(figsize, figsize))\n",
    "    plt.imshow(figure, cmap='Greys_r')\n",
    "    plt.show()\n",
    "\n",
    "# Plot latent space\n",
    "plot_latent_space(decoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸ§ª Experiment Instructions\n",
    "\n",
    "Tips: Refer to the visualization above and the loss output from your training cell (make sure `verbose` is set to 1 or 2) to answer the questions.\n",
    "\n",
    "1. **Adjust the weight of the KL divergence term** (`beta`) in the VAE loss function (e.g., set it to **0.1**, **1**, and **5**).  \n",
    "   Run the model with each weight setting and observe the results.\n",
    "\n",
    "2. **Change the dimensionality of the latent space** (`latent_dim`) to explore how it affects the modelâ€™s behavior.  \n",
    "   Try setting `latent_dim` to **2**, **4**, and **8**.  \n",
    "   Re-run your model for each setting and visualize:\n",
    "   - The reconstructed images  \n",
    "   - The latent space projection (for 2D, plot directly; for higher dimensions, use PCA or t-SNE)\n",
    "\n",
    "3. **Discussion:**  \n",
    "   Based on your observations:\n",
    "   - How does changing the KL weight (`beta`) affect the balance between reconstruction quality and latent space smoothness?  \n",
    "   - How does increasing the latent space dimensionality (`latent_dim`) affect the expressiveness of the model and the clarity of the reconstructions?  \n",
    "   - Were these effects consistent with your expectations? Why or why not?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ca1-dat300",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
